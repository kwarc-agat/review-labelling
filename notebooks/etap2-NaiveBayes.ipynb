{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwarc\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB, BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in my opinion , a movie reviewer's most import...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you can watch this movie , that is based on a ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is asking a lot to believe , and though i...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no heroes and no story are the main attributes...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is not an art movie , yet i saw it an art...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>the conventional wisdom is that movie sequels ...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>nicolas roeg's mesmerizing 1971 film walkabout...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>the movie air force one should require a docto...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>\" well , jones , at least you haven't forgotte...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>in a time of bloated productions where special...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5006 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating  label3  \\\n",
       "0     in my opinion , a movie reviewer's most import...     0.1       0   \n",
       "1     you can watch this movie , that is based on a ...     0.2       0   \n",
       "2     this is asking a lot to believe , and though i...     0.2       0   \n",
       "3     no heroes and no story are the main attributes...     0.2       0   \n",
       "4     this is not an art movie , yet i saw it an art...     0.2       0   \n",
       "...                                                 ...     ...     ...   \n",
       "5001  the conventional wisdom is that movie sequels ...     0.9       2   \n",
       "5002  nicolas roeg's mesmerizing 1971 film walkabout...     0.9       2   \n",
       "5003  the movie air force one should require a docto...     0.9       2   \n",
       "5004  \" well , jones , at least you haven't forgotte...     0.9       2   \n",
       "5005  in a time of bloated productions where special...     0.9       2   \n",
       "\n",
       "      label4  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "5001       3  \n",
       "5002       3  \n",
       "5003       3  \n",
       "5004       3  \n",
       "5005       3  \n",
       "\n",
       "[5006 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5006x41631 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 759861 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stop_words.ENGLISH_STOP_WORDS)\n",
    "X_counts = vectorizer.fit_transform(df['review'].values)\n",
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5006x41631 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 759861 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_counts)\n",
    "X_tf = tf_transformer.transform(X_counts)\n",
    "X_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label3 = df['label3'].values\n",
    "Y_label4 = df['label4'].values\n",
    "Y_label3_names = np.unique(Y_label3).tolist()\n",
    "Y_label4_names = np.unique(Y_label4).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_train, X_counts_test, Y_label3_counts_train, Y_label3_counts_test = train_test_split(X_counts, Y_label3, test_size=0.1)\n",
    "X_tf_train, X_tf_test, Y_label3_tf_train, Y_label3_tf_test = train_test_split(X_tf, Y_label3, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb_counts = MultinomialNB(alpha=0.5).fit(X_counts_train, Y_label3_counts_train)\n",
    "clf_nb_tf = MultinomialNB().fit(X_tf_train, Y_label3_tf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nb_counts = clf_nb_counts.predict(X_counts_test)\n",
    "pred_nb_tf = clf_nb_tf.predict(X_tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_counts - Y_label3: 0.6187624750499002\n",
      "X_tf - Y_label3: 0.5369261477045908\n"
     ]
    }
   ],
   "source": [
    "print(\"X_counts - Y_label3: \"+ str(np.mean(pred_nb_counts == Y_label3_counts_test)))\n",
    "print(\"X_tf - Y_label3: \"+ str(np.mean(pred_nb_tf == Y_label3_tf_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.51      0.57       113\n",
      "           1       0.54      0.60      0.57       187\n",
      "           2       0.69      0.69      0.69       201\n",
      "\n",
      "    accuracy                           0.62       501\n",
      "   macro avg       0.62      0.60      0.61       501\n",
      "weighted avg       0.62      0.62      0.62       501\n",
      "\n",
      "[[ 58  45  10]\n",
      " [ 22 113  52]\n",
      " [ 10  52 139]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_label3_counts_test, pred_nb_counts, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_counts_test, pred_nb_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       135\n",
      "           1       0.46      0.68      0.55       186\n",
      "           2       0.63      0.79      0.70       180\n",
      "\n",
      "    accuracy                           0.54       501\n",
      "   macro avg       0.36      0.49      0.42       501\n",
      "weighted avg       0.40      0.54      0.46       501\n",
      "\n",
      "[[  0 110  25]\n",
      " [  0 127  59]\n",
      " [  0  38 142]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwarc\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_label3_tf_test, pred_nb_tf, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_tf_test, pred_nb_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters ={'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average = 'macro'),\n",
    "           'recall': make_scorer(recall_score, average = 'macro'),\n",
    "           'f1': make_scorer(f1_score, average = 'macro')}\n",
    "clf = GridSearchCV(MultinomialNB(), tuned_parameters, scoring=scoring,\n",
    "                  cv=2, verbose=3, refit='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n",
      "[CV] alpha=0.01 ......................................................\n",
      "[CV]  alpha=0.01, accuracy=0.541, f1=0.532, precision=0.545, recall=0.527, total=   0.0s\n",
      "[CV] alpha=0.01 ......................................................\n",
      "[CV]  alpha=0.01, accuracy=0.525, f1=0.515, precision=0.530, recall=0.509, total=   0.0s\n",
      "[CV] alpha=0.1 .......................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.1, accuracy=0.569, f1=0.568, precision=0.570, recall=0.567, total=   0.0s\n",
      "[CV] alpha=0.1 .......................................................\n",
      "[CV]  alpha=0.1, accuracy=0.563, f1=0.558, precision=0.561, recall=0.556, total=   0.0s\n",
      "[CV] alpha=0.5 .......................................................\n",
      "[CV]  alpha=0.5, accuracy=0.600, f1=0.593, precision=0.608, recall=0.585, total=   0.0s\n",
      "[CV] alpha=0.5 .......................................................\n",
      "[CV]  alpha=0.5, accuracy=0.582, f1=0.572, precision=0.590, recall=0.565, total=   0.0s\n",
      "[CV] alpha=1.0 .......................................................\n",
      "[CV]  alpha=1.0, accuracy=0.585, f1=0.553, precision=0.620, recall=0.550, total=   0.0s\n",
      "[CV] alpha=1.0 .......................................................\n",
      "[CV]  alpha=1.0, accuracy=0.586, f1=0.557, precision=0.622, recall=0.552, total=   0.0s\n",
      "[CV] alpha=10.0 ......................................................\n",
      "[CV]  alpha=10.0, accuracy=0.506, f1=0.374, precision=0.330, recall=0.446, total=   0.0s\n",
      "[CV] alpha=10.0 ......................................................\n",
      "[CV]  alpha=10.0, accuracy=0.503, f1=0.368, precision=0.327, recall=0.444, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwarc\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\kwarc\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=MultinomialNB(),\n",
       "             param_grid={'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}, refit='f1',\n",
       "             scoring={'accuracy': make_scorer(accuracy_score),\n",
       "                      'f1': make_scorer(f1_score, average=macro),\n",
       "                      'precision': make_scorer(precision_score, average=macro),\n",
       "                      'recall': make_scorer(recall_score, average=macro)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_counts_train, Y_label3_counts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63       112\n",
      "           1       0.55      0.59      0.57       184\n",
      "           2       0.72      0.67      0.70       205\n",
      "\n",
      "    accuracy                           0.63       501\n",
      "   macro avg       0.64      0.63      0.63       501\n",
      "weighted avg       0.64      0.63      0.63       501\n",
      "\n",
      "[[ 70  34   8]\n",
      " [ 29 109  46]\n",
      " [ 10  57 138]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report:\")\n",
    "pred = clf.predict(X_counts_test)\n",
    "print(metrics.classification_report(Y_label3_counts_test, pred))\n",
    "print(metrics.confusion_matrix(Y_label3_counts_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - different types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.49      0.56       112\n",
      "           1       0.54      0.65      0.59       184\n",
      "           2       0.72      0.69      0.71       205\n",
      "\n",
      "    accuracy                           0.63       501\n",
      "   macro avg       0.64      0.61      0.62       501\n",
      "weighted avg       0.64      0.63      0.63       501\n",
      "\n",
      "[[ 55  47  10]\n",
      " [ 20 119  45]\n",
      " [  8  55 142]]\n"
     ]
    }
   ],
   "source": [
    "clf_multi = MultinomialNB().fit(X_counts_train, Y_label3_counts_train)\n",
    "pred_multi = clf_multi.predict(X_counts_test)\n",
    "print(metrics.classification_report(Y_label3_counts_test, pred_multi, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_counts_test, pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.25      0.30       112\n",
      "           1       0.43      0.52      0.47       184\n",
      "           2       0.56      0.56      0.56       205\n",
      "\n",
      "    accuracy                           0.48       501\n",
      "   macro avg       0.46      0.44      0.44       501\n",
      "weighted avg       0.47      0.48      0.47       501\n",
      "\n",
      "[[ 28  57  27]\n",
      " [ 24  96  64]\n",
      " [ 20  71 114]]\n"
     ]
    }
   ],
   "source": [
    "clf_gauss = GaussianNB().fit(X_counts_train.toarray(), Y_label3_counts_train)\n",
    "pred_gauss = clf_gauss.predict(X_counts_test.toarray())\n",
    "print(metrics.classification_report(Y_label3_counts_test, pred_gauss, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_counts_test, pred_gauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.66      0.64       112\n",
      "           1       0.58      0.47      0.52       184\n",
      "           2       0.67      0.76      0.71       205\n",
      "\n",
      "    accuracy                           0.63       501\n",
      "   macro avg       0.62      0.63      0.62       501\n",
      "weighted avg       0.62      0.63      0.62       501\n",
      "\n",
      "[[ 74  27  11]\n",
      " [ 33  86  65]\n",
      " [ 13  36 156]]\n"
     ]
    }
   ],
   "source": [
    "clf_compl = ComplementNB().fit(X_counts_train, Y_label3_counts_train)\n",
    "pred_compl = clf_compl.predict(X_counts_test)\n",
    "print(metrics.classification_report(Y_label3_counts_test, pred_compl, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_counts_test, pred_compl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.46      0.57       112\n",
      "           1       0.53      0.77      0.63       184\n",
      "           2       0.81      0.66      0.73       205\n",
      "\n",
      "    accuracy                           0.65       501\n",
      "   macro avg       0.70      0.63      0.64       501\n",
      "weighted avg       0.69      0.65      0.65       501\n",
      "\n",
      "[[ 51  60   1]\n",
      " [ 13 141  30]\n",
      " [  4  66 135]]\n"
     ]
    }
   ],
   "source": [
    "clf_bern = BernoulliNB().fit(X_counts_train, Y_label3_counts_train)\n",
    "pred_bern = clf_bern.predict(X_counts_test)\n",
    "print(metrics.classification_report(Y_label3_counts_test, pred_bern, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_counts_test, pred_bern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - different types with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       118\n",
      "           1       0.50      0.60      0.55       203\n",
      "           2       0.60      0.87      0.71       180\n",
      "\n",
      "    accuracy                           0.55       501\n",
      "   macro avg       0.37      0.49      0.42       501\n",
      "weighted avg       0.42      0.55      0.48       501\n",
      "\n",
      "[[  0  95  23]\n",
      " [  0 121  82]\n",
      " [  0  24 156]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwarc\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf_multi = MultinomialNB().fit(X_tf_train, Y_label3_tf_train)\n",
    "pred_multi = clf_multi.predict(X_tf_test)\n",
    "print(metrics.classification_report(Y_label3_tf_test, pred_multi, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_tf_test, pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.19      0.23       118\n",
      "           1       0.44      0.45      0.44       203\n",
      "           2       0.39      0.46      0.42       180\n",
      "\n",
      "    accuracy                           0.39       501\n",
      "   macro avg       0.37      0.37      0.37       501\n",
      "weighted avg       0.38      0.39      0.39       501\n",
      "\n",
      "[[23 45 50]\n",
      " [32 91 80]\n",
      " [25 72 83]]\n"
     ]
    }
   ],
   "source": [
    "clf_gauss = GaussianNB().fit(X_tf_train.toarray(), Y_label3_counts_train)\n",
    "pred_gauss = clf_gauss.predict(X_tf_test.toarray())\n",
    "print(metrics.classification_report(Y_label3_tf_test, pred_gauss, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_tf_test, pred_gauss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.02       118\n",
      "           1       0.50      0.51      0.50       203\n",
      "           2       0.56      0.89      0.69       180\n",
      "\n",
      "    accuracy                           0.53       501\n",
      "   macro avg       0.68      0.47      0.40       501\n",
      "weighted avg       0.64      0.53      0.45       501\n",
      "\n",
      "[[  1  87  30]\n",
      " [  0 104  99]\n",
      " [  0  19 161]]\n"
     ]
    }
   ],
   "source": [
    "clf_compl = ComplementNB().fit(X_tf_train, Y_label3_tf_train)\n",
    "pred_compl = clf_compl.predict(X_tf_test)\n",
    "print(metrics.classification_report(Y_label3_tf_test, pred_compl, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_tf_test, pred_compl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.34      0.46       118\n",
      "           1       0.54      0.75      0.63       203\n",
      "           2       0.74      0.67      0.70       180\n",
      "\n",
      "    accuracy                           0.62       501\n",
      "   macro avg       0.66      0.59      0.60       501\n",
      "weighted avg       0.65      0.62      0.62       501\n",
      "\n",
      "[[ 40  73   5]\n",
      " [ 13 152  38]\n",
      " [  3  56 121]]\n"
     ]
    }
   ],
   "source": [
    "clf_bern = BernoulliNB().fit(X_tf_train, Y_label3_tf_train)\n",
    "pred_bern = clf_bern.predict(X_tf_test)\n",
    "print(metrics.classification_report(Y_label3_tf_test, pred_bern, labels=Y_label3_names))\n",
    "print(metrics.confusion_matrix(Y_label3_tf_test, pred_bern))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
